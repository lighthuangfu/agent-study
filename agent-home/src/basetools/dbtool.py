# -*- coding: utf-8 -*-
import os
from uuid import uuid4
from datetime import datetime
from typing import List, Dict
from typing import List, Optional
from qdrant_client import QdrantClient
from qdrant_client import models as qmodels
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


# å°è¯•å¯¼å…¥ä¸åŒçš„ embedding æ¨¡å‹
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False

try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

def _get_qdrant_client() -> QdrantClient:
    """
    å†…éƒ¨å¸®åŠ©å‡½æ•°ï¼šæ„é€  QdrantClientã€‚
    é»˜è®¤è¿æ¥æœ¬åœ° 6333 ç«¯å£ï¼Œæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼š
    - QDRANT_URL
    - QDRANT_API_KEY
    """
    return QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)


# åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆä½¿ç”¨ LangChain çš„æ ‡å‡†åˆ†å‰²å™¨ï¼‰
_text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,           # æ¯ä¸ª chunk çš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=50,         # chunk ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼‰
    length_function=len,      # è®¡ç®—é•¿åº¦çš„å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "!", "?", " ", ""],  # åˆ†å‰²ç¬¦ä¼˜å…ˆçº§ï¼ˆä¸­æ–‡å‹å¥½ï¼‰
)


def _split_text_into_chunks(text: str, max_len: int = 500) -> List[str]:
    """
    ä½¿ç”¨ LangChain çš„ RecursiveCharacterTextSplitter åˆ‡åˆ†æ–‡æœ¬ã€‚
    
    å‚æ•°:
        text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
        max_len: æ¯ä¸ª chunk çš„æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    
    è¿”å›:
        æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
    """
    if not text:
        return []
    
    # å¦‚æœæŒ‡å®šäº†ä¸åŒçš„ max_lenï¼Œåˆ›å»ºæ–°çš„åˆ†å‰²å™¨
    if max_len != 500:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=max_len,
            chunk_overlap=min(50, max_len // 10),  # é‡å ä¸º chunk_size çš„ 10%ï¼Œæœ€å¤š 50
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "!", "?", " ", ""],
        )
        chunks = splitter.split_text(text)
    else:
        chunks = _text_splitter.split_text(text)
    
    return chunks


def _get_embedding_model() -> Embeddings:
    """
    è·å– embedding æ¨¡å‹å®ä¾‹ã€‚
    
    ä¼˜å…ˆçº§ï¼š
    1. å¦‚æœè®¾ç½®äº† EMBEDDING_MODEL ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
    2. å¦‚æœå¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨ HuggingFaceï¼ˆå…è´¹ï¼Œæœ¬åœ°è¿è¡Œï¼Œæ”¯æŒä¸­æ–‡ï¼‰
    3. å¦‚æœè®¾ç½®äº† OPENAI_API_KEYï¼Œä½¿ç”¨ OpenAI Embeddings
    4. å¦åˆ™å›é€€åˆ°ç®€å•çš„å“ˆå¸Œ embeddingï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
    
    è¿”å›:
        Embeddings å®ä¾‹
    """
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº† embedding æ¨¡å‹
    embedding_model = os.getenv("EMBEDDING_MODEL", "").lower()
    
    # æ–¹æ¡ˆ1ï¼šä½¿ç”¨ HuggingFaceï¼ˆæ¨èï¼Œå…è´¹ä¸”æ”¯æŒä¸­æ–‡ï¼‰
    if HUGGINGFACE_AVAILABLE and (embedding_model in ("", "huggingface", "hf")):
        try:
            # ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ä¸‹è½½
            # å¯é€‰æ¨¡å‹ï¼š
            # - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" (å¤šè¯­è¨€ï¼Œ384ç»´)
            # - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2" (å¤šè¯­è¨€ï¼Œ768ç»´)
            # - "shibing624/text2vec-base-chinese" (ä¸­æ–‡ä¸“ç”¨ï¼Œ768ç»´)
            model_name = os.getenv(
                "HUGGINGFACE_EMBEDDING_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            print(f"ğŸŸ¢ ä½¿ç”¨ HuggingFace Embedding æ¨¡å‹: {model_name}")
            return HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={"device": "cpu"},  # ä½¿ç”¨ CPUï¼Œå¦‚æœæœ‰ GPU å¯æ”¹ä¸º "cuda"
                encode_kwargs={"normalize_embeddings": True}  # å½’ä¸€åŒ–å‘é‡
            )
        except Exception as e:
            print(f"âš ï¸ HuggingFace Embedding åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•å“ˆå¸Œæ–¹æ³•")
    
    # æ–¹æ¡ˆ2ï¼šä½¿ç”¨ OpenAI Embeddingsï¼ˆéœ€è¦ API Keyï¼‰
    if OPENAI_AVAILABLE and (embedding_model in ("openai", "gpt")):
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("ğŸŸ¢ ä½¿ç”¨ OpenAI Embedding æ¨¡å‹")
            return OpenAIEmbeddings(
                model="text-embedding-3-small",  # æˆ– "text-embedding-3-large"
                openai_api_key=api_key
            )
        else:
            print("âš ï¸ æœªè®¾ç½® OPENAI_API_KEYï¼Œå›é€€åˆ°ç®€å•å“ˆå¸Œæ–¹æ³•")
    
    # æ–¹æ¡ˆ3ï¼šå›é€€åˆ°ç®€å•çš„å“ˆå¸Œæ–¹æ³•ï¼ˆä»…ç”¨äºæµ‹è¯•/æ¼”ç¤ºï¼‰
    print("âš ï¸ ä½¿ç”¨ç®€å•çš„å“ˆå¸Œ Embeddingï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰ï¼Œå»ºè®®é…ç½® HuggingFace æˆ– OpenAI")
    return _SimpleHashEmbeddings(dimension=384)


class _SimpleHashEmbeddings(Embeddings):
    """
    ç®€å•çš„å“ˆå¸Œ embedding å®ç°ï¼ˆä»…ä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰ã€‚
    è¿™ä¸æ˜¯çœŸæ­£çš„è¯­ä¹‰å‘é‡ï¼Œä»…ç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºã€‚
    """
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨ã€‚"""
        return [self._embed_text(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬ã€‚"""
        return self._embed_text(text)
    
    def _embed_text(self, text: str) -> List[float]:
        """å†…éƒ¨æ–¹æ³•ï¼šå°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚"""
        import math
        vec = [0.0] * self.dimension
        if not text:
            return vec
        
        for ch in text:
            idx = hash(ch) % self.dimension
            vec[idx] += 1.0
        
        # ç®€å•å½’ä¸€åŒ–
        norm = math.sqrt(sum(v * v for v in vec)) or 1.0
        return [v / norm for v in vec]


# åˆ›å»ºå…¨å±€ embedding å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_default_embedding: Optional[Embeddings] = None


def _get_default_embedding() -> Embeddings:
    """è·å–é»˜è®¤çš„ embedding å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ã€‚"""
    global _default_embedding
    if _default_embedding is None:
        _default_embedding = _get_embedding_model()
    return _default_embedding


def _simple_hash_embedding(text: str, dim: int = 384) -> List[float]:
    """
    æ–‡æœ¬å‘é‡åŒ–å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰ã€‚
    
    ç°åœ¨ä½¿ç”¨æˆç†Ÿçš„ embedding æ¨¡å‹ï¼ˆHuggingFace/OpenAIï¼‰ï¼Œè€Œä¸æ˜¯ç®€å•çš„å“ˆå¸Œã€‚
    
    å‚æ•°:
        text: è¦å‘é‡åŒ–çš„æ–‡æœ¬
        dim: å‘é‡ç»´åº¦ï¼ˆæ³¨æ„ï¼šå®é™…ç»´åº¦å–å†³äºä½¿ç”¨çš„ embedding æ¨¡å‹ï¼Œæ­¤å‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
    
    è¿”å›:
        å‘é‡åˆ—è¡¨
    """
    embedding = _get_default_embedding()
    return embedding.embed_query(text)


def _embed_documents(texts: List[str]) -> List[List[float]]:
    """
    æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£åˆ—è¡¨ï¼ˆæ›´é«˜æ•ˆï¼‰ã€‚
    
    å‚æ•°:
        texts: è¦å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
    
    è¿”å›:
        å‘é‡åˆ—è¡¨çš„åˆ—è¡¨
    """
    if not texts:
        return []
    embedding = _get_default_embedding()
    return embedding.embed_documents(texts)

def index_generated_doc_to_qdrant(
    text: str, 
    user_intent: str,
    collection_name: str = "generated_docs"
    ) -> str:
    """
    å°†ç”Ÿæˆçš„æ–‡æ¡£ç´¢å¼•åˆ° Qdrant å‘é‡æ•°æ®åº“ã€‚
    """
    if not text or not text.strip or not user_intent:
        return "ç”¨æˆ·æ„å›¾æˆ–æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç´¢å¼•ã€‚"

    print(f">>> [Index Generated Doc] å¼€å§‹ç´¢å¼•æ–‡æ¡£{text[:100]}...")
    print(f">>> [Index Generated Doc] ç”¨æˆ·æ„å›¾: {user_intent}")
    print(f">>> [Index Generated Doc] æ–‡æ¡£å†…å®¹: {text[:100]}...")
    print(f">>> [Index Generated Doc] é›†åˆåç§°: {collection_name}")
     # 1. åˆ‡ç‰‡
    chunks = _split_text_into_chunks(text, max_len=500)
    if not chunks:
        print(f">>> [Index Generated Doc] æ–‡æ¡£åˆ‡ç‰‡ç»“æœä¸ºç©ºï¼Œè·³è¿‡å‘é‡å…¥åº“ã€‚")
        return "æ–‡æ¡£åˆ‡ç‰‡ç»“æœä¸ºç©ºï¼Œè·³è¿‡å‘é‡å…¥åº“ã€‚"
    vectors = _embed_documents(chunks)
    doc_id = str(uuid4())
    now = datetime.utcnow().isoformat() + "Z"
    payloads: List[Dict] = []
    for idx, chunk in enumerate(chunks):
        payloads.append(
            {
                "doc_id": doc_id,
                "chunk_id": idx,
                "user_intent": user_intent,
                "source": "generated_doc",
                "created_at": now,
                "text": chunk,
            }
        )
     # 4. å†™å…¥ Qdrantï¼ˆå¯ä»¥å¤ç”¨ç°æœ‰ save_vectors_to_qdrantï¼Œæˆ–ç›´æ¥ç”¨ clientï¼‰
    client = _get_qdrant_client()
    dim = len(vectors[0])

    try:
        client.get_collection(collection_name)
    except Exception:
        client.recreate_collection(
            collection_name,
            vectors_config=qmodels.VectorParams(size=dim, distance=qmodels.Distance.COSINE),
        )

    ids = [str(uuid4()) for _ in vectors]
    points = [
        qmodels.PointStruct(id=pid, vector=vec, payload=pl)
        for pid, vec, pl in zip(ids, vectors, payloads)
    ]
    client.upsert(collection_name=collection_name, points=points)
    return f"å·²å°†æ–‡æ¡£ {doc_id} çš„ {len(points)} ä¸ªç‰‡æ®µå†™å…¥é›†åˆ `{collection_name}`ã€‚"


def query_by_doc_id(
    doc_id: str,
    collection_name: str = "generated_docs",
    limit: int = 100,
    with_vectors: bool = False,
) -> List[Dict]:
    """
    æŒ‰ doc_id ä» Qdrant é›†åˆä¸­æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰ç‰‡æ®µï¼ˆæŒ‰ chunk_id æ’åºï¼‰ã€‚

    å‚æ•°:
        doc_id: æ–‡æ¡£ IDï¼ˆå†™å…¥æ—¶ payload ä¸­çš„ doc_idï¼‰
        collection_name: é›†åˆåç§°ï¼Œé»˜è®¤ "generated_docs"
        limit: æœ€å¤šè¿”å›æ¡æ•°ï¼Œé»˜è®¤ 100
        with_vectors: æ˜¯å¦è¿”å›å‘é‡ï¼Œé»˜è®¤ False

    è¿”å›:
        åŒ¹é…çš„ point åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º {"id": ..., "payload": {...}, "vector": ...(å¯é€‰)}
    """
    client = _get_qdrant_client()
    try:
        client.get_collection(collection_name)
    except Exception:
        return []

    query_filter = qmodels.Filter(
        must=[
            qmodels.FieldCondition(
                key="doc_id",
                match=qmodels.MatchValue(value=doc_id),
            )
        ]
    )
    results, _ = client.scroll(
        collection_name=collection_name,
        scroll_filter=query_filter,
        limit=limit,
        with_vectors=with_vectors,
        with_payload=True,
    )
    # æŒ‰ chunk_id æ’åºï¼Œä¾¿äºè¿˜åŸæ–‡æ¡£é¡ºåº
    results.sort(key=lambda p: p.payload.get("chunk_id", 0))
    return [
        {
            "id": p.id,
            "payload": p.payload or {},
            **({"vector": p.vector} if with_vectors and p.vector else {}),
        }
        for p in results
    ]